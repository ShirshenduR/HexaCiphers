"""
Data Collection Module
Real data collection from various social media platforms using APIs
"""

import os
import tweepy
import logging
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

class DataCollector:
    """Class for collecting data from social media platforms"""
    
    def __init__(self):
        self.twitter_api = self._initialize_twitter_api()
    
    def _initialize_twitter_api(self):
        """Initialize Twitter API client"""
        try:
            # Twitter API v2 client
            bearer_token = os.getenv('TWITTER_BEARER_TOKEN')
            api_key = os.getenv('TWITTER_API_KEY')
            api_secret = os.getenv('TWITTER_API_SECRET')
            access_token = os.getenv('TWITTER_ACCESS_TOKEN')
            access_token_secret = os.getenv('TWITTER_ACCESS_SECRET')
            
            if not bearer_token:
                logger.warning("Twitter Bearer Token not found. Twitter API will not work.")
                return None
            
            # Initialize Twitter API v2 client
            client = tweepy.Client(
                bearer_token=bearer_token,
                consumer_key=api_key,
                consumer_secret=api_secret,
                access_token=access_token,
                access_token_secret=access_token_secret,
                wait_on_rate_limit=True
            )
            
            logger.info("Twitter API client initialized successfully")
            return client
            
        except Exception as e:
            logger.error(f"Failed to initialize Twitter API: {str(e)}")
            return None

    def collect_twitter_data(self, keywords: List[str], limit: int = 10) -> List[Dict[str, Any]]:
        """Collect tweets based on keywords using Twitter API v2"""
        try:
            if not self.twitter_api:
                logger.warning("Twitter API not available, using fallback data")
                return self._get_fallback_twitter_data(keywords, limit)
            
            # Construct search query
            query = " OR ".join([f'"{keyword}"' for keyword in keywords])
            query += " lang:en -is:retweet"  # English tweets, no retweets
            
            # Search for tweets
            tweets = self.twitter_api.search_recent_tweets(
                query=query,
                max_results=min(limit, 100),  # API limit is 100
                tweet_fields=['created_at', 'author_id', 'public_metrics', 'context_annotations'],
                user_fields=['username', 'public_metrics', 'verified'],
                expansions=['author_id']
            )
            
            if not tweets.data:
                logger.info("No tweets found for the given keywords")
                return self._get_fallback_twitter_data(keywords, limit)
            
            # Process tweets
            users = {user.id: user for user in tweets.includes.get('users', [])}
            processed_tweets = []
            
            for tweet in tweets.data:
                author = users.get(tweet.author_id)
                
                processed_tweet = {
                    'id': f"twitter_{tweet.id}",
                    'platform': 'Twitter',
                    'content': tweet.text,
                    'user_id': tweet.author_id,
                    'username': author.username if author else 'unknown',
                    'followers': author.public_metrics.get('followers_count', 0) if author else 0,
                    'created_at': tweet.created_at.isoformat() if tweet.created_at else datetime.utcnow().isoformat(),
                    'likes': tweet.public_metrics.get('like_count', 0),
                    'retweets': tweet.public_metrics.get('retweet_count', 0),
                    'replies': tweet.public_metrics.get('reply_count', 0),
                    'keywords': keywords
                }
                processed_tweets.append(processed_tweet)
            
            logger.info(f"Collected {len(processed_tweets)} tweets for keywords: {keywords}")
            return processed_tweets
            
        except Exception as e:
            logger.error(f"Error collecting Twitter data: {str(e)}")
            return self._get_fallback_twitter_data(keywords, limit)
    
    def _get_fallback_twitter_data(self, keywords: List[str], limit: int) -> List[Dict[str, Any]]:
        """Fallback data when Twitter API is not available - returns empty list for production"""
        logger.warning("Twitter API not available - no fallback data in production mode")
        return []

    def collect_reddit_data(self, subreddit: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Collect Reddit data - disabled in production (Twitter-only focus)"""
        logger.info("Reddit data collection disabled - Twitter-only monitoring in production")
        return []# Initialize global data collector instance
data_collector = DataCollector()

import os
import tweepy
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

class DataCollector:
    """Class for collecting data from social media platforms"""
    
    def __init__(self):
        self.twitter_api = self._initialize_twitter_api()
    
    def _initialize_twitter_api(self):
        """Initialize Twitter API client"""
        try:
            # Twitter API v2 client
            bearer_token = os.getenv('TWITTER_BEARER_TOKEN')
            api_key = os.getenv('TWITTER_API_KEY')
            api_secret = os.getenv('TWITTER_API_SECRET')
            access_token = os.getenv('TWITTER_ACCESS_TOKEN')
            access_token_secret = os.getenv('TWITTER_ACCESS_SECRET')
            
            if not bearer_token:
                logger.warning("Twitter Bearer Token not found. Twitter API will not work.")
                return None
            
            # Initialize Twitter API v2 client
            client = tweepy.Client(
                bearer_token=bearer_token,
                consumer_key=api_key,
                consumer_secret=api_secret,
                access_token=access_token,
                access_token_secret=access_token_secret,
                wait_on_rate_limit=True
            )
            
            logger.info("Twitter API client initialized successfully")
            return client
            
        except Exception as e:
            logger.error(f"Failed to initialize Twitter API: {str(e)}")
            return None
    
    def collect_twitter_data(self, keywords: List[str], limit: int = 10) -> List[Dict[str, Any]]:
        """Collect tweets based on keywords using Twitter API v2"""
        try:
            if not self.twitter_api:
                logger.warning("Twitter API not available, using fallback data")
                return self._get_fallback_twitter_data(keywords, limit)
            
            # Construct search query
            query = " OR ".join([f'"{keyword}"' for keyword in keywords])
            query += " lang:en -is:retweet"  # English tweets, no retweets
            
            # Search for tweets
            tweets = self.twitter_api.search_recent_tweets(
                query=query,
                max_results=min(limit, 100),  # API limit is 100
                tweet_fields=['created_at', 'author_id', 'public_metrics', 'context_annotations'],
                user_fields=['username', 'public_metrics', 'verified'],
                expansions=['author_id']
            )
            
            if not tweets.data:
                logger.info("No tweets found for the given keywords")
                return self._get_fallback_twitter_data(keywords, limit)
            
            # Process tweets
            users = {user.id: user for user in tweets.includes.get('users', [])}
            processed_tweets = []
            
            for tweet in tweets.data:
                author = users.get(tweet.author_id)
                
                processed_tweet = {
                    'id': f"twitter_{tweet.id}",
                    'platform': 'Twitter',
                    'content': tweet.text,
                    'user_id': tweet.author_id,
                    'username': author.username if author else 'unknown',
                    'followers': author.public_metrics.get('followers_count', 0) if author else 0,
                    'created_at': tweet.created_at.isoformat() if tweet.created_at else datetime.utcnow().isoformat(),
                    'likes': tweet.public_metrics.get('like_count', 0),
                    'retweets': tweet.public_metrics.get('retweet_count', 0),
                    'replies': tweet.public_metrics.get('reply_count', 0),
                    'keywords': keywords
                }
                processed_tweets.append(processed_tweet)
            
            logger.info(f"Collected {len(processed_tweets)} tweets for keywords: {keywords}")
            return processed_tweets
            
        except Exception as e:
            logger.error(f"Error collecting Twitter data: {str(e)}")
            return self._get_fallback_twitter_data(keywords, limit)
    
    def _get_fallback_twitter_data(self, keywords: List[str], limit: int) -> List[Dict[str, Any]]:
        """Fallback data when Twitter API is not available"""
        import random
        
        sample_tweets = [
            "India is making great progress in technology and innovation! #DigitalIndia #TechIndia",
            "Beautiful landscapes in Kashmir today #Kashmir #India #Nature", 
            "India's space program achievements are remarkable #ISRO #SpaceIndia",
            "Indian startup ecosystem is booming with new innovations",
            "India's democratic values and their global significance",
            "Amazing cultural diversity and heritage of India",
            "India's contribution to global peace and stability",
            "Technology advancements in Indian cities",
            "Traditional Indian festivals and celebrations",
            "India's economic growth and development"
        ]
        
        sample_users = [
            {"user_id": "user1", "username": "tech_enthusiast", "followers": 1500},
            {"user_id": "user2", "username": "india_lover", "followers": 2300},
            {"user_id": "user3", "username": "cultural_explorer", "followers": 850},
            {"user_id": "user4", "username": "news_reporter", "followers": 5000},
            {"user_id": "user5", "username": "travel_blogger", "followers": 1200},
        ]
        
        tweets = []
        for i in range(min(limit, len(sample_tweets))):
            user = random.choice(sample_users)
            tweet = {
                'id': f"fallback_twitter_{i+1}",
                'platform': 'Twitter',
                'content': sample_tweets[i],
                'user_id': user['user_id'],
                'username': user['username'],
                'followers': user['followers'],
                'created_at': (datetime.utcnow() - timedelta(hours=random.randint(1, 24))).isoformat(),
                'likes': random.randint(5, 100),
                'retweets': random.randint(0, 50),
                'replies': random.randint(0, 25),
                'keywords': keywords
            }
            tweets.append(tweet)
        
        return tweets
    
    def collect_reddit_data(self, subreddit, limit=10):
        """
        Simulate Reddit data collection
        
        Args:
            subreddit (str): Subreddit to collect from
            limit (int): Number of posts to collect
            
        Returns:
            list: List of simulated Reddit posts
        """
        logger.info(f"Simulating Reddit data collection from r/{subreddit}")
        
        posts = []
        for i in range(min(limit, len(self.sample_reddit_posts))):
            user = random.choice(self.sample_users)
            post = {
                "id": f"reddit_{i+1}",
                "platform": "Reddit",
                "user_id": user["user_id"],
                "username": user["username"],
                "content": self.sample_reddit_posts[i],
                "subreddit": subreddit,
                "created_at": (datetime.utcnow() - timedelta(hours=random.randint(1, 48))).isoformat(),
                "upvotes": random.randint(0, 200),
                "comments": random.randint(0, 50)
            }
            posts.append(post)
        
        return posts
    
    def collect_youtube_data(self, query, limit=10):
        """
        Simulate YouTube data collection
        
        Args:
            query (str): Search query
            limit (int): Number of videos to collect
            
        Returns:
            list: List of simulated YouTube posts
        """
        logger.info(f"Simulating YouTube data collection for query: {query}")
        
        sample_titles = [
            "India's Amazing Cultural Heritage",
            "Technology Innovation in India",
            "Propaganda Analysis: Anti-India Campaigns",
            "Indian Democracy and Its Challenges",
            "Economic Growth in Modern India",
            "Social Media Manipulation Tactics",
            "India's Role in Global Politics",
            "Combating Misinformation Online",
            "Indian Values and Global Influence",
            "Digital India Success Stories"
        ]
        
        posts = []
        for i in range(min(limit, len(sample_titles))):
            user = random.choice(self.sample_users)
            post = {
                "id": f"youtube_{i+1}",
                "platform": "YouTube",
                "user_id": user["user_id"],
                "username": user["username"],
                "title": sample_titles[i],
                "content": f"Video content about: {sample_titles[i]}",
                "views": random.randint(1000, 100000),
                "likes": random.randint(10, 1000),
                "duration": f"{random.randint(2, 30)}:{random.randint(10, 59)}",
                "created_at": (datetime.utcnow() - timedelta(days=random.randint(1, 30))).isoformat()
            }
            posts.append(post)
        
        return posts
    
    def simulate_real_time_feed(self, duration_minutes=5):
        """
        Simulate real-time data feed
        
        Args:
            duration_minutes (int): Duration to simulate in minutes
            
        Returns:
            list: List of posts with timestamps
        """
        logger.info(f"Simulating real-time feed for {duration_minutes} minutes")
        
        posts = []
        start_time = datetime.utcnow()
        
        for minute in range(duration_minutes):
            # Simulate 1-3 posts per minute
            posts_per_minute = random.randint(1, 3)
            
            for _ in range(posts_per_minute):
                platform = random.choice(["Twitter", "Reddit", "YouTube"])
                user = random.choice(self.sample_users)
                
                if platform == "Twitter":
                    content = random.choice(self.sample_tweets)
                else:
                    content = random.choice(self.sample_reddit_posts)
                
                post = {
                    "id": f"{platform.lower()}_{len(posts)+1}",
                    "platform": platform,
                    "user_id": user["user_id"],
                    "username": user["username"],
                    "content": content,
                    "created_at": (start_time + timedelta(minutes=minute)).isoformat(),
                    "engagement": random.randint(10, 500)
                }
                posts.append(post)
        
        return posts